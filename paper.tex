% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\usepackage{graphicx}
\usepackage{url}
\begin{document}

\title{Malicious Web Pages... or another reason to be sceptical of the e-commerce Directive}
\numberofauthors{3}
\author{
\alignauthor
Huw Fryer\\
       \affaddr{Web Science DTC}\\
       \affaddr{Electronics \& Computer Science}\\
       \affaddr{University of Southampton}\\
       \email{hf1g10@ecs.soton.ac.uk}
% 2nd. author
%\alignauthor
%Tim Chown\\
       % \affaddr{WAIS Group}\\
       % \affaddr{Electronics \& Computer Science}\\
       % \affaddr{University of Southampton}\\
       % \email{tjc@ecs.soton.ac.uk}
% 3rd. author
% \alignauthor Roksana Moore\\
%        \affaddr{ILAWS}\\
%        \affaddr{Faculty of Business \\and Law}\\
%        \affaddr{University of Southampton}\\
%        \email{roksana.moore@soton.ac.uk}
%no more       
}
\maketitle
\begin{abstract}
The huge increase of usage of the Web over the last 20 years has brought with it a considerable criminal element.  Much of this criminal infrastructure is built upon spreading malicious software and using it to extract credentials, or provide a cover for their illegal activities by making victim computers perform the criminal acts for them.  This paper will provide an explanation of some of how the problem has developed from using viruses in the 1980s, up to the current issue of drive-by downloads.  We will provide a review of what technology currently exists in seeking to minimise the problem, and why this is not enough.  Our proposal is that simple actions by companies providing services on the Web could significantly impact criminals' ability to profit from cybercrime.  We will further argue that laws excluding liability for intermediary such as the E-commerce Directive or s230 CDA in the USA reduces the incentives for the necessary effort by these companies and leads to an unsatisfactory state.



\end{abstract}

% A category with the (minimum) three required fields
\category{K.4.1}{Computing Milieux}{Public Policy Issues}[Abuse and crime involving computers]
\category{K.5.2}{Computing Milieux}{Governmental Issues}[Regulation]

\section{Introduction}
Computer fraud and abuse has progressed significantly in the last twenty years as use of the World Wide Web  has increased exponentially.  Previously, so-called ``hackers" were largely interested in kudos and recognition whereas now they have been replaced by organised crime who are intent on profit.  Conventional legal approaches to intermediary liability have emphasised the importance of not burdening the intermediaries with excessive liability and provides large exemptions in many areas.  The majority of crime is easily preventable if Web stakeholders adhered to best practice, but there is frequently no incentive for them to do so, because whilst the cumulative cost is significant, the cost to any individual company is minimal.  This cost includes direct costs to users or banks who are victim to malware, but also a not insignificant amount of advert ``clicks" are fraudulent.

This paper makes the case for increased liability for intermediaries to provide an adequate level of security online.  The area which will be explored is that of drive-by downloads, which is malicious Web pages which attempt to attack the browsers which visit them.  We start by explaining how malware has evolved over the last twenty years, to the situation which we currently have.  Technological solutions which seek to minimse the effect are then examined, and after concluding that they are insufficient, possible pre-emptive measures which could protect Web users from drive-by downloads are considered.  The advantages of different entities in accepting legal responsibility are discussed, and we conclude that hosting providers are probably in the best position to make a significant improvement to the level of security online.


\section{Evolution of Malware Propagation}
Like any other environment, that of information technology has been victim to criminality.  It has suffered worse than many, because of the automation which it facilitates mean that an attacker needs accomplish a task only once such that it can be repeated many times -- and the cumulative effect can be devastating.  For example, consider the fact that the volume of spam has reduced considerably over the last few years, such that now it represents \textbf{only} 75.2\% of all email traffic, compared to 92.6\% in 2008\cite{trustwave}.  The main way that criminal groups are able to maintain infrastructure which can send this volume of spam, and commit other crimes is through the use of malicious software (malware).  This allows victims to be recruited into a botnet, and perform criminal tasks for the attacker without their knowledge; but can also be used to steal credentials to exfiltrate money from the victim's account.

This section provides some background of this evolution, up to the primary focus of the paper: that of ``drive-by" downloads.  Thee distinctions between different types of malware are often unhelpful, since a lot of them do not fit neatly into one category, and incorporate elements of different types of malware.  The reason for the distinctions in this section is to emphasise the differences in propagation methods, and the differences in strategy which are required to combat them.  We will also seek to introduce some terminology which we feel will be helpful to readers unfamiliar with the technical elements of Web security.

\subsection{Exploitation vs Social Engineering}
In order to work, malware needs to be able to run on a victim machine.  One method is known as \textbf{social engineering} which is to simply make the user voluntarily run the malicious code.  This can be accomplished through the use of \textbf{Trojan} style malware.  Like the name suggests, this is a reference to the Trojan horse from Greek legend, which was let into Troy and allowed the Greeks hiding within to sneak out and open the gates of the besieged city from the inside.  In the context of security, this might comprise an application purporting to perform a certain task, whilst at the same time an application hidden within would simultaneously attempt to subvert the machine it was run on.

Another method is to \textbf{exploit a vulnerability} on the machine.  A \textbf{vulnerability} is a flaw, or ``bug" in a piece of software which amounts to a security weakness.  Vulnerabilities will have a greater or lesser degree of severity, but the most serious are those which allow Remote Code Execution (RCE).  These vulnerabilities allow an attacker to run their own code rather than the code intended by the application.  This is done by confusing the program into accepting user input as commands to be executed rather than as data to be manipulated.  An \textbf{exploit} is a piece of code which takes advantage of the vulnerability, in order to run the desired code.  In traditional computer based applications, this will be done by corrupting the memory, but in Web applications there are many other methods with which this can be achieved, and other less serious vulnerabilities which exist.  These will be discussed in section \ref{sec:exploitation}.  

Exploitation can be used together with social engineering.  For example, a common attack vector is that of a malicious attachment sent in an email.  Once the user executes the attachment, then it can be used to take over the computer.  In order for it to be plausible for the user to open the atachment, then some social engineering will be required.  A typical example is an email from purporting to be from a delivery company, with information in a PDF file the location of a parcel sent to the victim.  The victim opens the attachment, and then the maliciously crafted PDF document exploits a vulnerability in the PDF reading software to run its desired code (known as the \textbf{payload}).

\subsection{Viruses}
A dominant vector early on, was that of computer viruses, now incorrectly a layperson's term for all forms of malware.  A \textbf{virus} was characterised by the fact that it would attach itself to a previously benign file, and then spread from file to file on the computer.  The originally came from Von Neumann's work on self-replicating code, and then Cohen analysed the properties of computer viruses in more detail.  He considered a biological analogy in that a virus was simpler than most other organisms, and had the ability to reproduce, therefore it could be classified as ``alive"\cite{cohen}.  The nature of the spread of real world viruses, and actions to limit them, appeared to hold with computer viruses too\cite{murray1988}, which led to a substantial body of work on epidemiology of computer networks.

Any sharing of an infected file between computers would cause the infected file to begin its spread on the files of the other computer it got shared with.  This was the most logical way for it to spread, since use of networks, and in particular the Web, was in its infancy.  Instead, propagation relied upon physical sharing of floppy disks or other storage media.  Although they could cause programs to function incorrectly, and on some occasions caused significant problems (to the extent of physically destroying computers), often they were not intended to be malicious, and were pieces of software created largely for fun and kudos within the hacker community.

\subsection{Worms}
Like viruses, not all worms were intended to be malicious.  An early worm, known as the ``Morris Worm" in 1988 caused some significant problems in attempting to measure how many machines were connected to the Internet.  However, the big explosion of the use of worms began in the early 2000s.  Like a virus, a worm self-propagates, but will exploit a vulnerability to install itself on a machine and then self-propagate to other machines rather than between files on the same machine.  Rather than relying on physical devices to propagate, it automates the process through using network or Internet connections.  It required no intervention from the user, it would simply scan for vulnerable machines and exploit the same vulnerability on each one.  This mechanism is known as \textbf{push} type propagation.  As usage of the Web began to grow exponentially, it became incredibly effective as a propagation method, because few of the computers connecting to the Internet had adequate security to deal with these attacks.  Firewalls were not commonly installed, meaning that it was possible to push this malware without any barriers, and many computers were directly accessible on the Internet to an attacker.  Operating system vendors also took time to adjust to the nature of the threat in updating their products, for example it was not until 2003 that Microsoft introduced a regular patching cycle, and even then the update mechanism required users to opt-in rather than be done automatically.

This attack method has since fallen out of favour, and there are several possible reasons for this.  The first, is that operating system vendors have caught up with the threats from the nature of the Internet, and have introduced additional security and updates into their products.  As such, worm exploits -- which attacked operating systems -- are not so easy to find.  Modern operating systems also have firewalls installed by default, which solves the problem of machines being directly vulnerable to the threat.  Similarly, the depletion of IP addresses also led to the adoption of Network Address Translation (NAT) hardware, which enabled multiple computers on a local network to share the same IP address on the Internet.  A side effect of this is that the NAT hardware will not accept unsolicited communications from the Internet, and this can block worm based attacks.

Another possible reason is the change of the nature of attackers.  Cybercrime moved from a (mostly) harmless pursuit by computer enthusiasts, to a more profit-driven method dominated by organised crime.  A worm is ``noisy" -- that is, it spreads aggressively from machine to machine, and is easy to spot because it causes a spike in the amount of traffic on a network.  In a culture where this was done for fun, or to make a point, this is not a problem.  Several of the famous worms of the mid 2000s contained messages, whether they be ``Insert witty message here", or messages to Microsoft founder Bill Gates to improve the security of his [Windows] software.  By contrast, the modern breed of attacker wants to use the infected machine to make profit from it, and frequently the best way of doing this is to infect it silently.  A popular method of doing this is through drive-by downloads.  That is not to say that worms weren't also used in this way, merely that it is plausible that an attacker wishes to keep it more hidden than worm based propagation would allow.

\subsection{Drive-by Downloads}
Rather than the push based worm malware, drive-by downloads wait for the victim to come to them.  The process is to create a malicious Web page, which will exploit vulnerabilities on the users' computer automatically once it is visited.  This overcomes some of the limitations of worm based malware, in that it is stealthy since it is indistinguishable from general Web browsing; and also it increases the potential victim base since it allows a way through the user's firewall\cite{provos}.  The software which initiates the connection to the Web page is the browser, so often browser vulnerabilities will be targeted to attempt to run an exploit on the computer.  This might be done through using JavaScript to corrupt the browser itself, or will use one of the \textbf{plugins} which the browser is running.  A plugin is an additional feature added to the browser, usually to play multimedia content -- common targets are Flash, Adobe Reader or Java.  Like with operating systems, these will also contain vulnerabilities, so the attacker will attempt to exploit these in the same way to get their malicious code to run.

The phenomenon of drive-by downloads is not a new one, but it has increased to become a significant threat .  Provos et al performed a detailed analysis of drive-by attacks as early as March 2006-07\cite{provos}, and also January - October 2007\cite{provos2008}.  They found 1.3\% of results in Google search results were malicious; and 0.6\% of the most popular 1 million URLs had, at some point, been used as malicious hosting.  The format of a drive by download has been to compromise a legitimate site through either outdated software; unsanitised user input; or adverts\cite{provos}\cite{provos2008}.  These legitimate sites will usually contain some code to redirect the browser to a malicious site which contains the actual malware payload.

There are advantages to an attacker in compromising a legitimate website as opposed to buying cheap throwaway domain names.  The existing reputation contained by a legitimate website means that it is harder to shut down than a malicious site, and also it becomes more likely that a potential victim will visit it.  A legitimate website will already have a certain amount of traffic, and this can be enhanced through taking advantage of trending topics.  Major news events will lead to a lot of people searching for information about it, so to control a website which relates to that is an advantage.  A recent example could be news about the missing M3H70 Malaysian Airlines flight.  Moore et al. found evidence of this practice with advert filled and malicious sites effectively exploiting trending terms on both Google and Twitter generating considerable profits for criminals\cite{fashion}.  

Finally, if a website with high reputation is compromised, it can be used to facilitate blackhat search engine optimisation (SEO).  The way the Google ranking system works assigns weight to the links to a website based on the rank of the website which links to it, so a website with high reputation could significantly enhance the ranking of another website.  Mention John et al. paper about blackhat SEO and heuristics?!!!

Therefore, in order to successfully attack a user's computer in a drive-by attack using this strategy, the following prerequisites are required: 
\begin{enumerate}
	\item A previously benign website needs to be taken over and redirect to an attack site;
	\item A user with a vulnerable Web browser (or browser plugin) has to visit it;
	\item The malware has to successfully exploit that vulnerability and remain hidden.
\end{enumerate}

This means that there are two main strategies to limit the effect of drive-by downloads on the Web are to prevent the successful exploitation of users visiting the website; and preventing websites from becoming compromised in the first place.  These two will be considered in turn in the next section.


\section{Minimising the Effect}
\label{sec:technical}
The majority of the literature has focused on mitigating the effect of drive-by downloads to the clients, after the initial compromise (to the website) has already happened.  

The identification of a malicious page enables a warning to be placed to warn users from visiting it, or to prevent the execution of the malicious code if they visit it anyway.  To identify pages in advance, a \textbf{client honeypot} would typically be used.  A honeypot has traditionally been a server, which appears to be deliberately vulnerable in order to induce attackers to try and exploit them.  These vary in their level of interaction from simply reporting the types of attacks which they are seeing (low interaction) or by attempting to gather more detail about the strategies being used by interacting with the attacker (high interaction).  A \textbf{client honeypot} relies on the same principle, except it mimics the client and searches out malicious pages.  These may be simply classified as malicious or not; or they could allow the download of the malicious file to see what the effect is on the computer.  Provos et al. used high interaction honeypots for their investigation into the level of malicious pages described earlier, which looked for any changes to the state of the machine; suspicious redirects, or suspicious downloads\cite{provos2008}.

Pages identified by Google are presented with a warning when in the search results through the safe-browsing API, which is also used by browsers such as Google Chrome and Mozilla Firefox.  In the event that a user tries to browse to a malicious page, they are given further prompts to attempt to prevent them from going onto the page.  Despite general criticism in the literature about the effectiveness of browser warnings, a recent study by Akhawe and Felt suggested that these warnings are actually effective, with only between 9\% and 23\% of users going through malware or phishing warnings\cite{akhawe2013}.

That said, the use of client honeypots or pre-emptive observation in this way does have certain limitations.  Firstly, the browser which is being simulated might not be the target of the malware, such as if a honeypot used a version of Internet Explorer when the malware targeted Mozilla Firefox.  It may not attempt to execute in situations like that.  Similarly, there is ``IP Centric" malware, which would only appear to users with certain IP addresses.  An IP address can identify which network a user is coming from, and with security companies or search engines having known IP address ranges, then (depending on the level of control over the page) the malware could decline to activate if an adversary visited the site.

In visiting a Web page (whether pre-emptively or in realtime) then it is necessary to identify whether the page is malicious.  The techniques for doing this have varied, with tradeoffs between the accuracy and processing time required to classify as malicious or benign.  One possible solution is to use  anomaly detection methods to identify malicious characteristics, such as a large amount of redirects, or certain Web programming techniques\cite{cova2010detection}.  Some of the identification techniques can be used in advanced or in real time, as a browser plugin.  Machine learning techniques have been used with systems like \texttt{Cujo}\cite{rieck2010cujo}, \texttt{ZOZZLE}\cite{curtsinger2011zozzle}, and \texttt{SurfGuard}\cite{sachin2012surfguard}.  

A similar idea has been to examine the behaviour of the browser to prevent certain patterns.  This has the advantage that there is no need for the technique to actually have any knowledge about how the malicious code is constructed, but simply relies upon the observed effects of the browser or operating system of visiting the page from the operating system itself.  One way this can work is through relying on the fact that RCE requires memory corruption to occur, and then for the attacker's code (known as \textbf{shellcode}) to be executed.  Egele et al. sought to identify shellcode in output from the Web page, and prevent it from running\cite{egele2009mitigating}.  Other techniques have been to examine the scenarios related to the download and execution of executables from visiting a page.  In the event that these characteristics would suggest they have been downloaded without consent, they could simply be prevented from executing\cite{hsu2011browserguard}\cite{lu2010blade}.

Alternatively, the heuristics of the page or the Web server itself, rather than its content can also be used as another means of identification.  For example, pages which have suddenly changed in character to be making use of black hat search engine optimisation (SEO) techniques are likely to be malicious, and can therefore be identified as such.  Whilst blocking based on these criteria would be easy to get around, it would make the pages less likely to be viewed, because the SEO would be less effective!\cite{john}.  Zhang et al. sought to identify compromised Web pages through their links to attack servers hosting the malicious content, which as attackers currently work, is usually hosted on a different server.  Through combining knowledge of IP addresses and domains related to those particular servers, then a reasonable network of compromised pages could be identified\cite{zhang2011arrow}.


\section{Exploiting a Website}
\label{sec:exploitation}
The previous section considered the problem from the clients point of view: preventing them from visiting the websites in the first place, and if they do visit them seeking to prevent the execution of the malicious code.  The other side which can be considered is to prevent the websites from becoming exploited.  This section will consider how websites are exploited, and what can be done to prevent this from occuring.

Websites are made by getting various applications to integrate, and as such are vulnerable to similar attacks that conventional applications are.  The code required to get a website running is split into different parts.  The Web server is responsible for accepting the requests coming from the client, and responding with the content.  Frequently, the server will interact with the database, which allows functions such as login to take place, and the display of dynamic pages.  These will both also be running on an operating system.  The display the users see is done using HyperText Markup Language (HTML) and Cascading Style Sheets (CSS), which the user's Web browser understands and can render.  Creating content using HTML is frequently automated so that novice users can add content, and this is done by a content management system (CMS), which will present the operator with a WYSIWYG interface, and store the content in a database which can be retrieved and displayed when it is requested by a user.

\subsection{OWASP Top 10}
The interaction between these different applications means that there are many different types of vulnerabilities which can be present in a Web application.  Vulnerabilities in components related to the Web server are pretty similar to vulnerabilities suffered on a home computer, since they run on an operating system, and like all software, will have bugs and vulnerabilities.  The Web application itself has to interact with all these, and this can cause difficulties, as input can be manipulated to be commands hidden as data.  The Open Web Application Security Project (OWASP) compiles a list which is frequently referred to as the most common vulnerabilities seen in Web applications.  They are ranked not just by how common they are, but also by the potential impact, and how easy they are to exploit.  The list below is the current iteration of their Top 10 list:

\begin{enumerate}
       \item Injection
       \item Broken Authentication and Session Management
       \item Cross-Site Scripting (XSS)
       \item Insecure Direct Object References
       \item Security Misconfiguration
       \item Sensitive Data Exposure
       \item Missing Function Level Access Control
       \item Cross-Site Request Forgery (CSRF)
       \item Using Components with Known Vulnerabilities
       \item Unvalidated Redirects and Forwards
\end{enumerate}

Injection based attacks (no. 1) typically (though not always) target the database, by embedding database commands (SQL) into input.  Rather than the application correctly separating the content from the commands, it interprets them together, and runs the commands which the user has put in.  Since the content of Web pages is frequently obtained from the database (e.g. if it's using a CMS), being able to modify this could facilitate placing malicious content onto the Web page.  It is this is the technique which is often used to steal information from a database, such as personal information.  More seriously, Microsoft SQL Server has got a command called \texttt{xp\_cmdshell} which interprets the input as functions to be run on the operating system itself, and so theoretically enabling the whole server to be taken over and not just the database.  

Another form of attacks are attacks against the Web browser which connects to the website, or the user themself.  XSS (no. 2) is commonly used here, where code like JavaScript is inserted into a page, and executes every time someone visits it.  JavaScript is used in all modern browsers, and the Web would not function without it, but it represents a possibility for corrupting the browser's code and becoming a RCE vulnerability.  Alternatively, including content from other openly malicious website could enable an attack against one of the browser's plugins to run, for example a maliciously crafted Flash file.  Alternatively, it could simply redirect the browser to a different page entirely, either openly malicious or a phishing page to steal credentials.  Unvalidated redirects (no. 10) are not an attack in themselves, but they facilitate other attacks against the users.  Web pages will frequently redirect their users, e.g. to track people who open an email, but if this page redirects based on (unchecked) input it could send the user to other websites.  It increases the likelihood of a user clicking on a link, because it appears to be going to a known or trusted website, and so could be used as part of a phishing attack.

The other main class of attacks is that of privilege escalation, which is where a user seeks to gain additional privileges, particularly those with administrative power.  These are possible when there are elements of the code which are not specifically kept from the website visitors, and which could be accessed by simply changing a few things.  For example, changing a URL from \texttt{www.example.com?user=bob} to \texttt{www.example.com?user=admin} providing access to administrator privliges (no. 4).  Alternatively, if the access control system in general is not configured properly (no. 7), then it could be exploited, for example by intercepting communications of administrative users to get their password.  Naturally, attacks against the user from the previous page could be used as part of privilege escalation as well.  If a sufficiently high privilege level is obtained, then this can be used to place malicious content on the Web page.

Security misconfiguration, and use of components with known vulnerabilities (no. 5 and no. 9) are related, and might often encompass many of the other vulnerabilities described in the list.  A misconfigured server would include elements of the server which have been improperly set, and therefore enable other attacks to take place.  It might include weak passwords, inappropriate permissions, or server rules which are in general too permissive.  Allowing too many permissions could be used alongside a file upload vulnerability, which allows an attacker to upload a file without checking what type of file it is.  This amounts to a remote code execution vulnerability, because placing a file on the server and then visiting that Web page in a browser would cause the code within it to be run on the server.

Using vulnerable components would be an example of a misconfigured website, and the vulnerability could be any of the ones described above, and include any part of the setup -- server, Web application or CMS.  All software will have occasional bugs or vulnerabilities, and those which are actively maintained will fix the vulnerabilities once they are reported.  However, in order to have an effect on the website itself, these updates have to be installed.  It is possible to identify old versions of particular software -- on the server by using port scanning tools like nmap; or CMS versions by checking for the existence of various files.  In the event that an attacker discovers an old version of some software exists, then they can seek to exploit that rather than expending effort to find any new weaknesses.

Automated scans are made by criminals to detect vulnerable websites and to seek in exploiting them.  One strategy is the use of ``Google dorks", which is to use search terms characteristic of vulnerable components, or a website which is already compromised.  This was demonstrated by Clayton \& Moore, who showed that there was a correlation between these search terms and the compromise of websites for phishing attacks\cite{mooreClayton}.  For example, searching for \texttt{phpizabi 0.848b c1 hgp1} which would return websites powered by an old version of phpizabi, which contained a vulnerability allowing the upload of files to the server (vulnerability CVE-2008-0805)\footnote{This search no longer returns any results, 24 April 2014}.  Websites which were already compromised might have an uploaded ``shell", which is a piece of functionality designed so that the attacker can perform tasks on the server.  The phrase \texttt{inurl:c99.php} which would locate websites which had any URL containing c99.php, which is a popular shell used by attackers and demonstrated that the site was already compromised.

Vulnerable websites are at constant risk of compromise, and it is simply a matter of time before the compromise happens.  There is a wide range of uses a website can be put to, demonstrated by Canali \& Balzarotti\cite{canali2013}.  They deployed web based honeypots to analyse exactly what an attacker would do following a successful exploitation.  They used 500 websites with different characteristics, mostly based around vulnerable CMS software.  In the event that the attacker could upload a shell, then they would on 46\% of occasions, and then use that to log on average after 3 and a half hours.  Only 1.1\% of attackers specifically sought to add a drive-by download onto the website, but nearly 49.4\% attempted to gain more permanent control of the machine, and 27.7\% tried to get the machine into an IRC botnet\footnote{IRC stands for Internet Relay Chat, and used to be a common way for attackers to control botnets.  Since many server administrators will simply block any network traffic connecting to this, it has become a lot rarer}, which would also enable drive-by downloads alongside other more general malicious activity such as phishing, sending spam, or hosting illegal content.

Somewhat concerning, is the apparent prevalence of vulnerabilities from the OWASP top 10 on the Web.  WhiteHat security's 2013 Global Security report found that 86\% of websites had a ``serious" vulnerability on their website, defined as the ability to compromise at least part of their site\cite{whitehat}.  Trustwave's Global Security Report 2013, found that 38\% of domains used Password1 as a password, and also gave a breakdown of application vulnerabilities (see \ref{tab:trustwave}).  Checkmarx analysed the source code of the most popular WordPress plugins, and found that 20\% of them, and 70\% of the most popular e-commerce plugins, contained these vulnerabilities.  Although these are all security vendors, possibly with their own interests in presenting the data in certain ways, the amount of data breaches which have occurrred do lend credence to their figures as to the seriousness of the situation.

\begin{table}
       \label{tab:trustwave}
       \caption{Top 10 Application Vulnerabilities, from \cite{trustwave}}
       \begin{tabular}{|c|p{5cm}|c|} 
       \hline
       1 & SQL Injection & 15\% \\
       \hline
       2 & Miscellaneous Logic Flaws & 14\% \\
       \hline
       3 & Insecure Direct Object Reference & 28\% \\
       \hline
       4 & Cross-Site Scripting (XSS)  & 82\% \\
       \hline
       5 & Failure to Restrict URL Access & 16\% \\
       \hline
       6 & Cross-Site Request Forgery & 72\% \\
       \hline
       7 & Other Injection &  7\% \\
       \hline
       8 &  Insecure File Uploads & 10\% \\
       \hline
       9 & Insecure Redirects & 24\% \\
       \hline
       10 & Various Denial of Service & 11\% \\
       \hline
       \end{tabular}
\end{table}

\subsection{Malicious Advertising}
//TODO: Write about this!!!  Don't need to say much, just how it works and why I'm not looking at it in great detail.

\section{Proactive Solutions}
The solutions described so far from the literature have considered how we could react to a drive-by download after it has already occurred.  Given the huge amount of websites, and even larger amount of users on the Web, even a small failure rate is likely to lead to a significant amount of compromised users.  This section concentrates on seeking to prevent drive-by downloads from occuring by ensuring that vulnerable software is patched, and the updates are deployed.

On the client side, modern software has increasingly started to automatically upgrade in order to fix vulnerabilities.  Microsoft's ``patch Tuesday" provides monthly updates to Windows and other software on the second Tuesday of every month, and both Adobe and Oracle have quarterly update cycles for their products.  Web browsers also update automatically, Internet Explorer as part of patch Tuesday, Google Chrome updates silently, and Mozilla Firefox automatically searches for updates and installs the update when the browser is restarted.  This means that common attack vectors are usually plugged pretty quickly, which suggests that the majority of the victims are either running old versins of the software prior to the introduction of automatic updates, or are deliberately preventing the updates from occurring.

On the server side, updates are slightly more problematic, since any change brings with it the danger of breaking website functionality, and appropriate due diligence in testing updates can take manpower.  Like with normal users, the low barrier to entry for running a website (e.g. using a CMS) means that the operators themselves are unaware of the risks of running outdated software.  Actively maintained CMSs like Wordpress or Drupal are periodically updated to fix bugs or vulnerabilities, but the high volume of websites which use them (WordPress is said to run 20\% of the Web) means that an attack against one website will work against many websites, making them an attractive target for attackers.  In an attempt to limit the issues relating to this, WordPress recently introduced a feature which automatically provides security and maintenance updates for versions 3.7 and above.  

Hosting providers who provide a platform with a Web server and database should ensure that they keep these up to date, and usually it will be within their interests to do so.  They will frequently provide a user interface to make it easier to install and update software like WordPress, or send emails to let them know when an update is released.  It is also possible for a website operator to hire ethical hackers known as \textbf{penetration testers} to probe their Web application for vulnerabilities and to explain how they could be fixed.  This ensures that problems are found before they are exploited by attackers, and can be easily fixed, but requires the operator of the website to have sufficient resources to hire a penetration tester, and know how vulnerabilities can cause problems.  Dutch hosting company Antagonist provides a free vulnerability scanning service to their customers which does the same thing, and also fixes them for free.

Whilst the approach by Antagonist is a promising idea which should be more widely adopted, there are limitations to simple scans like this.  Type I and type II errors can both cause problems.  Possible type I errors would require human intervention and discussion with the website operator to be sure it is a type I error.  This could add significantly to the staffing costs of running a Web hosting company.  Similarly, sometimes, it is necessary to craft a very specific attack against the website in order for it to succeed, and an automated tool would not be able to pick it up -- leading to a type II error.  Should a company wish to limit the amount of false positives, then naturally this kind of error is likely to increase.  Doing scans could also cause complacency by the website operators as to the security of their website, in viewing security as a product rather than a process.  WhiteHat security hypothesised that this was a reason for a higher level of vulnerability in websites which used static source code analysis tools compared to those who didn't.


\section{Entities Involved with Proactive Defence}
The nature of the problem of drive-by downloads is that it is an externality: the people who cause the problem are generally not affected by the consequences.  The compromised website will not be affected from a business point of view, since the purpose of a drive-by page is that it is not noticed.  When it is picked up by a blacklist, it is possible to remove the immediate issue without fixing the problem itself. It is also a problem with public good characteristics, in that fixing it costs the Web as a whole a considerable amount of money, but comparatively little to most individuals.  

For example, one common use for malware is to install a keylogger in order to steal money from bank accounts.  Even in the event that a user has a keylogger installed on their machine, banks will generally accept the loss for small amounts, because it is cheaper than fighting.  This cumulatively adds up to a significant financial drain.  Similarly, another common use for botnets is click fraud, which is using the victim machine to fraudulently click on pay per click adverts.  It is estimated that a significant percentage of all advertising revenue is fraudulent in this way, costing potentially \pounds{billions} every year.  Neither user or website are affected by this sufficiently to have an incentive to invest in removing the problem.

This section considers who the entities involved in a drive-by download process are, and what kinds of solutions they could offer in a more proactive way to actively try and prevent, or at least minimise, the issue of drive-by downloads.  The typical process is described in figure \ref{fig:drive-by}, and it is within this framework we will consider the issue of who should be legally responsible.  The solutions described in section \ref{sec:technical} make some progress, but there is no legal framework obligating the different entities to do anything.  Of the entities in the diagram, obviously, the criminal can be excluded from any legal framework since by definition they are unlikely to accept responsibility.  Similarly, Anarchania represents a jurisdiction with lax cybersecurity laws, so the bulletproof hosting company are also excluded.  That leaves consideration down to: the user; the user's ISP; the search engine; the website; and the hosting provider.

\begin{figure}
\centering
       \includegraphics[width=70mm]{drive-by-diagram.png}
       \caption{The process of a drive-by download}
       \label{fig:drive-by}
\end{figure}

Some of these have contested definitions, so for the avoidance of any doubt the terms will be used in the following way.  An ISP is the operator who physically provides access for a customer to access the Internet.  Hosting providers offer many different types of packages for people who wish to host a website, from managing everything including the CMS (like \url{http://wordpress.com}), to simply providing hardware and letting the customer do the rest.  In this analysis, the hosting provider is envisaged as a company who is responsible for day to day running of the Web server, which could be the website operator themselves or a specialist company.  Companies who simply provide an area for customers to store data such as Dropbox or Spideroak are excluded from consideration.  The phrase intermediaries is used to describe every other Internet service provide (i.e. companies providing a service on the Internet).

The first one to be analysed, that of ISPs will consider preventing clients from becoming infected; the rest will consider the servers.

\subsection{Monitoring by ISPs}
The notion of an ISP performing an intervention on behalf of the customers has been considered in the past, and is generally regarded as an effective strategy in minimising the effect of users participating in botnets.  Unfortunately, an ISP has got limited incentives to perform any security actions on behalf of their user because the market is based on thin margins and is characterised by information asymmetries, such that users are not prepared to pay more for security\cite{clayton}.  There are a few documented examples, however, which have had greater or lesser levels of success:

Dutch ISPs made an agreement between themselves that they would quarantine devices they believed to be infected.  Research by van Eeten found that they succeeded in contacting approximately only 10\% of their infected customers.  IETF RFC 6561 was written to provide advice about how an ISP might detect and remediate infections amongst their customers.  A significant difficulty, they observed, was the ability to get in touch with customers where none of these methods are guaranteed to be 100\% successful and that each has its own set of limitations".  For example, an email might be quick and possible to automate, but there is no guarantee that it would be read or not, or whether due to spam filters; users not using that account, or simply ignoring it.  On the other hand, blocking Internet access would alert the user to the problem, but there may not be anything they could do about it, such as if there was more than one machine on the network, or the infected device didn't have any interface with which to solve the problem.

The way in which an ISP would identify infected customers would be retrospective, once they noticed the customer's computer communicating with the botnet C\& C server, or with notification from another organisation about one of their customers performing some malicious acts.  This is possible because a customer's Internet traffic is all routed through the ISP, enabling them to see the destinations of their requests.  This does not necessarily require deep packet inspection (DPI) because the very fact that they are communicating with a known malicious server is a good indication of infection.  However, a likely consequence is that information about the user would need to be retained, such as a method of linking back the IP address to the customer; or sharing data with other organisations in order to detect infections.  For example, there might be lists of IP addresses (sometimes regarded as PII in certain circumstances) of known infected devices; or IP addresses which have sent spam.

That solution is also not very proactive, in that it waits for the machine to get infected and begin communicating with the C \& C server.  There are other approaches that an ISP could take, in physically preventing their users from visiting infected pages.  There is precedent for this in the UK, with websites containing child pornography, or court orders on ISPs to block access to websites which facilitate filesharing.  A recent agreement between the government, and ISPs in the UK has been that ISPs require an ``opt-in" before they will display (legal) pornographic websites.  

Aside from censorship issues, the nature of drive-by downloads means that it becomes more complicated to block them.  Because the majority of websites which contain a drive-by download are legitimate websites, the nature of the sites can change from benign to malicious on a regular basis, requiring the ISP to continually check these websites for malicious content.  This is different from filesharing or pornographic websites which will retain the same category in general and change less often.  Legitimate websites are generally unlikely to want to have ISPs continually checking them since this will use bandwidth, and offer them no value.  In the event that ISPs were to use a list like Google Safe Browsing (see below), then they are performing no additional service that a search engine could not provide.

Another more proactive approach, is that of posture checking the devices which attempt to connect to the Internet, and denying them access in the event that they are not regarded as being sufficiently secure.  This is known as Network Access Control (NAC), or Network Endpoint Assessment(NEA) and has been used in corporate networks, particularly where employees might use their device on other networks which may expose them to security threats.  At RSA 2009, Scott Charney of Microsoft suggested that this was something which could apply to the whole Internet, and infected devices should be treated like people with an infectious disease, and used a broader public health analogy of ``collective defense" as a means of protecting the Internet.  Such a solution would be unlikely to be popular (especially if we expand it to consider vulnerable devices rather than those which have already been exploited), and could cause significant problems to customers who do not have the technical skills to remediate the device themselves.  

\subsection{Monitoring by Search Engines}
Search engines provide listings of websites in order of their relevance to the search terms provided by the user.  The most popular search engine is Google, with close to 90\% of the search engine market in the UK.  Their ``PageRank" algorithm gives the rank of a website by working out the amount of other Web pages linking to it; but also by the quality of those web pages.  In order to do this, the search engine will send crawlers to every website it has listed and traverse through the links on the Web page.  A website can specifically register to be indexed in this way, or if there are enough links to a certain page then that can be enough to be included in the search engine results.

A search engine does have the incentive to eliminate drive-by downloads, because they reduce the quality of the search results, and consequently damage the product they are attempting to sell\cite{edwards2012}.  A lot of work is already done by these companies, for example, the results presented by Provos\cite{provos} are from Google.  Google also provide a ``Safe browsing" service, which allows developers to check pages they are about to visit to see if that page is listed as being malicious.  This is also integrated into some browsers, like Firefox and Chrome.  Given this, and the fact that they already visit the vast majority of websites anyway, it would appear that search engines are best placed to detect drive-by downloads (or vulnerable websites), but there are some limitations.

Like any analysis of malware, a search engine needs to choose between false positive and false negative errors.  The consequences of falsely declaring a website to be malicious could potentially be serious, so search engine providers need to be conservative about which pages they choose to classify like as malicious.  An illustration of this can be seen by the case of Spamhaus (a company responsible for compiling anti-spam lists) had to fight a five year legal battle to defend against a defamation claim.  The plaintiff, a ``marketing company" claimed that Spamhaus defamed them by classifying them as a spammers.  Classification like this would still leave the website available for people to visit, and even with the low of 9\% suggested by Akhawe\cite{akhawe2013} this is still a not inconsiderable amount of traffic on a popular website.

The sanction a search engine could apply instead is to include elements of the security posture as part of their ranking algorithms.  This was a suggestion by Edwards et al., which they referred to as ``depreferencing"\cite{edwards2012}.  The proposal there was that a search engine could run a greater risk of false positives, and minimise the traffic to a website and hence the exposure.  It is something which Google did do in a related area in 2011, where their algorithm was changed to limit the exposure of advert filled websites\cite{fashion}.  This is something which could also be applied to vulnerable websites, whether that's through outdated server software or application layer vulnerabilities.  As Moore et al. showed, higher exposure in trending topics and search results is a good thing for attackers\cite{fashion}, so to limit that exposure is likely to reduce the users infected.  Edwards et al. do warn about depreferencing though, that increasing tolerance of false positives could lead to the system being gamed where business rivals seek to plant malicious code on their rivals websites\cite{edwards2012}.

In relation to finding already compromised websites, it would be possible for malware to hide from a search engine in two different ways.  IP centric malware which was discussed earlier is one limitation, and the fact that there are parts of the website they cannot see.  There is a protocol, though not officially recognised it is commonly followed, of keeping a page called \texttt{robots.txt} in the main folder of a website (like \texttt{www.example.com/robots.txt}) which indicates which automated scanners are permitted to visit the website; and which pages of the website they are allowed to visit.  A scanner would identify itself through its user agent, for example \texttt{Googlebot} or \texttt{Bingbot} identify Google or Bing the two main search engines.  On most occasions, the website operators would be keen to have these search engines visiting their website, but in the event that the do not then the search engines would follow the protocol and decline to visit certain pages.  Pages which the search engines do not see would not necessarily be high in the rankings, but could still be used to host malware linked to from emails or social media.

\subsection{Monitoring by Hosting Provider}
Typically, the responsibility for managing the website and running the Web server will be split.  Often the people who are concerned with the content of the website will not have much technical knowledge.  As such, it has been shown that following the exploitation of one of these vulnerabilities, and the subsequent compromise of the website, it can take a while for them to notice the problems and fix them.  StopBadware and Commtouch conducted a survey of website owners whose sites were compromised, finding that 63\% of their sample didn't know about how their website got compromised, and only 6\% were able to identify the fact that their website had been compromised.  Almost half were only notified when they were faced with a browser warning screen.  Regarding the recovery, some 26\% of the websites remained compromised, and only 46\% were able to solve the problem themself\cite{stopBadware}.

Arguably, the hosting providers are in a better position than the website operators to discover the vulnerabilities.  There are different models which a hosting provider can offer to a customer, from simply renting out the hardware, to fully managing the content management system (CMS).  Often the model will be that the hosting provider will provide the hardware and the operating system, and grant the customer permissions to host their website on that.  These are often shared between many customers on the same physical hardware.  This model means that the hosting providers have a greater amount of priviliges, and therefore control over running processes, and hence are in a better position to run software to monitor changes to the filesystem or uploads to the server.  Firewalls can also be deployed, both against the infrastructure, and against the Web application itself.  Web application firewalls will seek to block malicious traffic characteristic of the vulnerabilities described above.  Technical reasons aside, it is also more efficient to have only a single entity running the server, rather than every single customer also attempting to do the same.

Whether hosting providers are currently doing a good enough job is open to question.  From the responses received by StopBadware in their survey, there was general dissatisfaction at the lack of support from the hosting provider following their website becoming compromised.  Some believed that the host was directly at fault, in that by becoming compromised themselves that led to al the customers on the server being compromised as well.  This was a motivation behind a study by Canali, Balzarotti and Francillon, who examined the ability of popular Web hosts to deal with attacks and compromises\cite{canaliHostingProviders}, and appeared to show that the security protection on offer is inadequate.  They created websites at a series of different providers, and simulated different types of attacks on them in order to see how the hosts responded.  Whilst some providers were able to block or mitigate some of the attacks, none of them provided complete protection.  Whilst some were better than others in terms of the protection they offered their users, none of them were able to protect against all the attacks the researchers simulated.  Of those who failed to mitigate these attacks, they made an abuse report, which is a complaint to the hosting provider about inappropriate content on the websites.  Of those, only 50\% replied to the report, and only one chose to notify the website owner.

It is proposed that Web hosting providers perform vulnerability scans on the websites which they host, to determine which are vulnerable, similar to the way Antagonist are beginning to roll out\cite{antagonist}.  In the event that a website is found to be vulnerable, then the operator should be notified and given an opportunity to fix it.  This might be as simple as upgrading the CMS, to completely changing the code used to communicate with the database.  In the event that the operator chooses not to respond, or does not make a fix quickly enough, then the access to this site should be blocked.  This could be done by DNS, so that the hostname doesn't work, but the operator could use the IP address to connect to the server and fix it.  Automated scans will not catch all vulnerabilities of course, but a website which is not vulnerable to any of these will be significantly harder for an attacker to successfully compromise, thereby reducing the incentive.

The advantage of making hosting providers responsible rather than search engines, is that they have physical control over the content which is hosted on their servers.  This means that they can, if they choose, block anyone from visiting a vulnerable or a compromised page.  The concentration of hosting providers \url{http://www.webhosting.info/webhosts/tophosts/global/} is such that only a few hosting providers would be required to adopt any set of best practices.


\subsection{Entities to be Excluded}
In many senses, it is the users faults that their machines get compromised, due to their poor security practices.  This might include use of outdated software (to the same effect as the websites), or declining to use anti-virus software or firewalls.  On some occasions, users will actively choose to ignore warnings and view a page known to be malicious.  Factors like this mean that many in the security industry agree with McGraw \& Felten's oft quoted comment that ``given a choice between dancing pigs and security, users will pick dancing pigs every time"\cite{akhawe2013}.  There is some literature suggesting that users be personally liable for the damage they cause through having unsecured machines\cite{deguzman}.  This, it is argued, would soon require users to invest properly in securing their machines therefore reducing the amount of damage they can cause.  This does not represent the whole problem, however.  Herley, for example argued that many of the security warnings presented to them are protecting against theoretical rather than actual problems, and that adopting them would lead to a considerable loss in terms of time outstripping the potential loss from an attack.  He also pointed out, that the ``dancing pigs" comment is unfair, in that users not actually offered ``security" but rather are offered a set of complicated guidelines for managing risk\cite{herley}.  Adams \& Sasse argue that it is these policies, which are incompatible with working practices, which is what causes bad security decisions rather than the users themselves.  When they can see the rationale behind security requirements with well-designed software, their security practices are good\cite{adamsSasse}.  

Alternatively, one might view website operators themselves as being responsible.  Like users they are frequently blameworthy for failing to take adequate security precautions and allowing their websites to become compromised.  A requisite standard of security practice could be proposed, and in the event that the operator falls below that, then they could be liable.  This might be useful as a supplemental approach, but it is not enough on its own.  In the event that a website operator is negligent and content remains online, ex post damages do not take away the problem of the damage it caused in the first place.  A further objection to obligating either of these groups to take action emerges from legal/economic theory: it is inefficient.  A far more efficient option would be to consider an entity with control over either of them to be responsible.  This would enable better identification of problematic nodes; and the deeper pockets of those entities would provide economies of scale to fix the problem -- much like some of the arguments in favour of vicarious liability\cite{atiyah}.

\section{Conclusions}
Lorem ipsum dolur sit amet.





\bibliographystyle{abbrv}
\bibliography{references}
%\clearpage
%\begin{landscape}
%\includegraphics{class-diagram.png}
%\end{landscape}
\balancecolumns
% That's all folks!
\end{document}
